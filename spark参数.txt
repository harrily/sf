spark.sql.result.partition.ratio = 0.8
spark.port.maxRetries = 999
spark.rss.client.retry.max = 10
spark.serializer = org.apache.spark.serializer.KryoSerializer
spark.sql.warehouse.dir = hdfs://sfbdp1/user/hive/warehouse
spark.sql.optimizer.maxIterations = 1000
spark.yarn.jars = local:/app/spark/lib/spark-assembly.jar
spark.driver.host = cnsz20pl4292
spark.sql.hive.convertMetastoreParquet = false
spark.history.fs.logDirectory = hdfs://cfo/log/spark3
spark.eventLog.enabled = true
spark.proxy.enable = true
spark.shuffle.manager = org.apache.spark.shuffle.RssShuffleManager
spark.executor.memoryOverhead = 1G
spark.sql.adaptive.enabled = true
spark.proxy.user = hive
spark.rss.remote.storage.path = hdfs://dw/unifflenew/data
spark.driver.maxResultSize = 2G
spark.rss.data.replica.read = 2
spark.sf.queue-grayer.db.driver = com.mysql.jdbc.Driver
spark.driver.port = 41799
spark.shuffle.service.enabled = false
spark.rss.client.shuffle.data.distribution.type = LOCAL_ORDER
spark.sql.adaptive.autoBroadcastJoinThreshold = 32MB
spark.driver.extraLibraryPath = /app/hadoop/lib/native
spark.sql.storeAssignmentPolicy = LEGACY
spark.sf.client.flag = 
spark.ui.filters.skip = true
spark.yarn.queue = root.predict_model
spark.jars = file:/HDATA/1/federal-server-8767/runner/federal-runtime-spark-runner-0.1-SNAPSHOT-jar-with-dependencies.jar
spark.kryoserializer.buffer.max = 1111m
spark.yarn.historyServer.address = http://cnsz26plr3lp:18080
spark.sf.new.queue.check = false
spark.app.name = SparkRunnerServer_new_688736_6b7ab933-023a-42ca-bb5c-b7ba664b735d
spark.rss.client.send.threadPool.size = 2
spark.sql.adaptive.coalescePartitions.parallelismFirst = false
spark.sf.userId = hive
spark.sf.sql.supportEnv = true
spark.network.timeout = 36000s
spark.sql.analyzer.maxIterations = 1000
spark.rpc.lookupTimeout = 36000s
spark.rss.data.replica.write = 2
spark.rpc.askTimeout = 36000s
spark.sql.adaptive.advisoryPartitionSizeInBytes = 128M
spark.driver.memory = 2g
spark.executor.instances = 1
spark.submit.pyFiles = 
spark.sf.queue-grayer.db.password = BD_1234pp
spark.rss.data.replica = 3
spark.hadoop.hive.exec.dynamic.partition.mode = nonstrict
spark.dynamicAllocation.maxExecutors = 400
spark.sf.queue-checker = com.sf.bdp.spark.QueueChecker
spark.app.startTime = 1685556810404
spark.executor.id = driver
spark.yarn.config.replacementPath = {{HADOOP_COMMON_HOME}}/../../..
spark.custom.function.mark = 1
spark.sf.queue-grayer.db.url = jdbc:mysql://bdpp-M.db.sfdc.com.cn:3306/bdpp
spark.sf.queue.gray.enable = true
spark.sql.legacy.timeParserPolicy = LEGACY
spark.driver.extraJavaOptions = -Dtopic=6b7ab933-023a-42ca-bb5c-b7ba664b735d -DlogTarget=Console
spark.app.initial.jar.urls = spark://cnsz20pl4292:41799/jars/federal-runtime-spark-runner-0.1-SNAPSHOT-jar-with-dependencies.jar
spark.submit.deployMode = client
spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.RM_HA_URLS = CNSZ17PL1791:8088,CNSZ17PL1792:8088
spark.master = yarn
spark.yarn.archive = hdfs://sfbdp1/spark/spark-3.2.1/spark-jars-rss-0.7-20230310.zip
spark.rss.storage.type = MEMORY_LOCALFILE_HDFS
spark.hadoop.mapreduce.input.fileinputformat.input.dir.recursive = true
spark.ui.filters = org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
spark.rss.client.assignment.shuffle.nodes.max = 10
spark.sf.queue-grayer.db.userName = bdpp
spark.executor.extraLibraryPath = /app/hadoop/lib/native
spark.executor.memory = 8g
spark.driver.extraClassPath = /app/federal-server-8767/runner/federal-runtime-spark-runner-0.1-SNAPSHOT-jar-with-dependencies.jar
spark.local.dir = /tmp/sparklocal
spark.eventLog.dir = hdfs://cfo/log/spark3
spark.dynamicAllocation.enabled = true
spark.sql.catalogImplementation = hive
spark.rpc.io.connectionTimeout = 36000s
spark.executor.cores = 2
spark.sql.parquet.writeLegacyFormat = true
spark.driver.appUIAddress = http://cnsz20pl4292:4055
spark.sql.adaptive.coalescePartitions.minPartitionNum = 1
spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS = cnsz17pl2288
spark.dynamicAllocation.minExecutors = 1
spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES = http://cnsz17pl2288:54315/proxy/application_1684325517623_5722210
spark.sql.extensions = org.apache.kyuubi.sql.KyuubiSparkSQLExtension
spark.dynamicAllocation.executorIdleTimeout = 20s
spark.rss.client.io.compression.codec = ZSTD
spark.sf.queue-grayer = com.sf.bdp.spark.QueueGrayer
spark.app.id = application_1684325517623_5722210